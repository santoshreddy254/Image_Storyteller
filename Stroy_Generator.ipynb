{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import warnings\n",
    "from sentence_embeddings.config import *\n",
    "import tensorflow as tf\n",
    "from sentence_embeddings.model import layers\n",
    "import nltk\n",
    "import json\n",
    "import itertools\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import numpy as np\n",
    "import random\n",
    "from word_embeddings.predict import Prediction\n",
    "from char_embeddings.preprocess import sample\n",
    "from tensorflow.keras.models import load_model\n",
    "import re\n",
    "from char_embeddings.preprocess import preprocess_data, generate_one_hot_encoding\n",
    "import json\n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary limited by K is : {'the': 23040, 'half': 181, 'ling': 22, 'book': 129, 'one': 1274, 'in': 5185, 'fall': 56, 'of': 7967, 'igneeria': 30, 'series': 29, 'kaylee': 2, 'soderburg': 2, 'copyright': 10, '2013': 1, 'all': 1202, 'rights': 10, 'reserved': 10, 'isbn': 3, '1492913731': 1, '13': 7}\n"
     ]
    }
   ],
   "source": [
    "embedding = \"word\"\n",
    "if embedding == \"character\":\n",
    "    char_indices = {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n",
    "                    'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, \n",
    "                    'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, \n",
    "                    'y': 25, 'z': 26}\n",
    "    indices_char = {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', \n",
    "                    10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', \n",
    "                    19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
    "    chars = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n",
    "             'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
    "    print(\"Character indices/tokenizer: \",char_indices)\n",
    "elif embedding == \"word\":\n",
    "    tokenizer_path = '/home/smuthi2s/perl5/NLP/Image_Storyteller/word_embeddings/working/tokenizer_30000.json'\n",
    "    with open(tokenizer_path) as f:\n",
    "        data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    N = 20    \n",
    "    out = dict(itertools.islice(tokenizer.word_counts.items(), N))\n",
    "    print(\"Dictionary limited by K is : \" + str(out))\n",
    "elif embedding == \"sentence\":\n",
    "    tokenizer_path = \"/home/smuthi2s/perl5/NLP/Image_Storyteller/sentence_embeddings/new_tokenizer_2000.json\"\n",
    "    with open(tokenizer_path) as f:\n",
    "        data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    N = 20    \n",
    "    out = dict(itertools.islice(tokenizer.word_counts.items(), N))\n",
    "    print(\"Dictionary limited by K is : \" + str(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A unknown man standing in the ground and the saw the only thing was a man who had been a few of a man who had been in the world and he had been in the world and he was a little of his family who had been in the world and he had been in the world and he was a little of his best friend in a world and he was a man who had been a few inches of the best in the world was a man who had been in the world and he had been in the world and\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"sentence\":\n",
    "    text = \"start with\"\n",
    "    text_2 = \"this is just great seth shouted .\"\n",
    "    idx2word = {v:k for k,v in tokenizer.word_index.items()}\n",
    "    encoded_data = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded_data_2 = tokenizer.texts_to_sequences([text_2])[0]\n",
    "    max_length = 6\n",
    "    total_sentences = [encoded_data,encoded_data_2]\n",
    "    model = layers.skip_thoughts(thought_size=thought_size, word_size=embed_dim, vocab_size=10000,\n",
    "                                 max_length=max_length)\n",
    "    checkpoint_dir = \"/scratch/smuthi2s/NLP_data/logs\"\n",
    "    checkpoint_path = \"/scratch/smuthi2s/NLP_data/logs_480/200_ckpt-50\"\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    lengths = np.array([len(encoded_data),len(encoded_data_2)])\n",
    "    tf.train.load_checkpoint(latest)\n",
    "    for i in range(10):\n",
    "        # print(total_sentences[-2:])\n",
    "        padded_data = pad_sequences(total_sentences[-2:],maxlen = max_length,padding='pre')\n",
    "        masked_prev_pred, masked_next_pred = model(padded_data,lengths)\n",
    "        total_sentences.append(list(np.argmax(masked_next_pred,axis=2)[0]))\n",
    "        lengths = np.append(lengths,len(np.argmax(masked_next_pred,axis=2)[0]))\n",
    "    story=''\n",
    "    for idx,i in enumerate(total_sentences):\n",
    "        i = np.unique(i)\n",
    "        if idx>0:\n",
    "            for j in i:\n",
    "                if j!=0:\n",
    "                    story+=idx2word[j]+\" \"\n",
    "    print(story)\n",
    "elif embedding == \"word\":\n",
    "    max_length = 179\n",
    "    pred = Prediction(tokenizer,max_length)\n",
    "    checkpoint_path = \"/home/smuthi2s/perl5/NLP/Image_Storyteller/word_embeddings/working/lang_model_30000.h5\"\n",
    "    pred.load_model(checkpoint_path)\n",
    "    print(pred.predict_sequnce(\"A unknown man standing\",100))\n",
    "elif embedding == \"character\":\n",
    "    checkpoint_path = \"/home/smuthi2s/perl5/NLP/Image_Storyteller/char_embeddings/char_story_generator_20100.h5\"\n",
    "    model = load_model(checkpoint_path)\n",
    "    variance = 0.5\n",
    "    maxlen = 40\n",
    "    sentence = 'a boy'\n",
    "    generated = ''\n",
    "    original = sentence\n",
    "    window = sentence\n",
    "    # Predict the next 400 characters based on the seed\n",
    "    for i in range(100):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(window):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, variance)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        window = window[1:] + next_char\n",
    "\n",
    "    print(original + generated)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf-2",
   "language": "python",
   "name": "tf-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
